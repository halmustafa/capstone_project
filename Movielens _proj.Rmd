---
title: "MovieLens.Recommendation systems project"
author: "Hussin Almustafa"
date: "2023-08-13"
output: pdf_document
---
 ### Introduction:
 
 a Recommender system can be found in almost every information-intensive website. For example, a list of likely preferred products are recommended to an customer when browsing the target product in Amazon , when watching a video clip in Youtube, a recommender system employed in the system suggests some relevant videos to users by learning the users’ behaviours that were generated previously. Here we provide the basics of how these recommendations are predicted,motivated by some of the approaches taken by the winners of the Netflix challenge.
 
 
 
 
[here is a like to my github repo](https://github.com/halmustafa/capstone_project.git)


### Methods :
after downloading the data i,m going to explore it, extract the Information from it ,then split the data into training and test sets to test my algorithm. then i will use the data to build and train a machine learning algorithm , using the inputs in one subset to predict Movie ratings in the validation set,by building a several models and comparing them using root mean squared error RMSA as loss function.


#### loding data 

####  Create Train and Final Hold-out Sets:


```{r}
#########################################################
# Create edx and final_holdout_test sets 
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = 
                                           "http://cran.us.r-project.org")
## Loading required package: tidyverse

if(!require(caret)) install.packages("caret", repos =
                                       "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
 

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file),
                                   fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file),
                                  fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
## Warning in set.seed(1, sample.kind = "Rounding"): non-uniform 'Rounding'
## sampler used
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1,
                                  p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
## Joining with `by = join_by(userId, movieId, rating, timestamp, title, genres)`
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



```



### Data exploration :

```{r}
head(edx)
```


```{r}
str(edx)
```


data.frame: 9000055 obs. of 6 variables:

We can see the number of unique users that provide ratings and for how many unique movies they provided them and how many genres


```{r}
edx %>% summarize(n_users = n_distinct(userId),
                  n_movies = n_distinct(movieId),
                  n_genres = n_distinct(genres))
```



#### movies distrbution:


```{r}
edx %>%
  dplyr::count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins= 30 ,color = "green",fill="blue") + 
  scale_x_log10() +
  ggtitle("Users")+
  theme_light()
```



#### rating distrbution:


```{r}
 edx %>%
  ggplot(aes(rating, y = after_stat(prop))) +
  
  geom_bar(color = "green", fill = "blue") +
  labs(x = "ratings" ) +
 ggtitle("Ratings")+
  theme_light()
```



####  most rating movies are :


```{r}
 edx %>%
  group_by(title) %>%
  summarize(count = n()) %>%
  arrange(-count) %>%
  top_n(30, count) %>%
  ggplot(aes(count, reorder(title, count))) +
  geom_bar(color ="green", fill = "blue", stat = "identity") +
  xlab("Count") +
  ggtitle("Most rating Movies")+
  theme_light()
```



####  Rating distribution by users :

```{r}
edx %>% group_by(userId) %>%
  summarize(count = n()) %>%
  ggplot(aes(count)) +
  geom_histogram(bins=30 ,color = "green", fill = "blue") +
  xlab("Ratings") +
  ylab("Users") +
  scale_x_log10() +
  ggtitle("Ratings distribution vs  users")+
  theme_light()
```

 
 
####  here is the matrix for a random sample of 100 movies and 100         users:


```{r}
users <- sample(unique(edx$userId), 100)
 edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% select(sample(ncol(.), 100)) %>% 
  as.matrix() %>% t(.) %>%
  image(1:100, 1:100,. , xlab="Movies", ylab="Users")
```



```{r}
 set.seed(1, sample.kind="Rounding")

 #edx_test will be 20 % of edx  data

 test_index <- createDataPartition(y = edx$rating, times = 1,
                                   p = 0.2, list = FALSE)
 edx_train <- edx[-test_index,]
 temp <- edx[test_index,]
 
# #  To make sure we don't include users and movies in the test set that do not
 # appear in the training set, we removed these using the semi_join function
 
 edx_test <- temp %>%
   semi_join(edx_train, by = "movieId") %>%
   semi_join(edx_train, by = "userId")
 
 # Adding back rows into edx_train set
 removed <- anti_join(temp, edx_test)
 
 ## Joining with `by = join_by(userId, movieId, rating, timestamp, title, genres)`
 edx_train <- rbind(edx_train, removed)
 
 rm( temp, removed)
```


#### To compare different models we will use root mean squared error (RMSE) as our loss function:


```{r}
 RMSE <- function(true_ratings, predicted_ratings){
   sqrt(mean((true_ratings - predicted_ratings)^2))
 }
```


### Building the Recommendation System:


#### model-1 Just the average :
the simplest possible recommendation system. We’re going to predict the same rating for all movies, regardless of the user and movie.


$$Y_{u, i} = \mu + \epsilon_{u,
      i}$$

 
```{r}
 mu_hat <- mean(edx_train$rating)
 mu_hat
```
 
 
####  create a table that’s going to store the results that we obtain as we go along. We’re going to call it rmse_results.


```{r}
#compute the residual mean squared error on the edx_test set

 naive_rmse <- RMSE(edx_test$rating, mu_hat)
 naive_rmse
```
 



```{r}
 rmse_results <- tibble(method = "Just the avg", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```
 
 
####  model-2 Movie Effect:

We can improve our model by adding a term b_i that represents the average rating for movie i


$$ Y_{u, i} = \mu +
b_i + \epsilon_{u,
i}$$
 
 
 
```{r}
 bi <- edx_train %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu_hat))


```

 
```{r}
 bi %>% ggplot(aes(b_i)) + 
  geom_histogram( bins= 20,color="green",fill="blue")+
  ggtitle("Movie effect hist")+
  theme_light()
```
 
 
```{r}
 predicted_ratings <- mu_hat + edx_test %>% 
  left_join(bi, by="movieId") %>%
  pull(b_i)
```
 
 
 
 
```{r}
 movie_rmse <- RMSE(predicted_ratings, edx_test$rating)
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Avg+Movie Effect",
                                     RMSE = movie_rmse ))

rmse_results %>% knitr::kable()
```
 
 
####  model_3 user effect:
because different users different in terms of how they rate movies We can further improve our model by adding b_u , the user-specific effect:


$$Y_{u, i} = \mu + b_i + b_u +
\epsilon_{u, i}$$




```{r}
edx_train %>% 
  group_by(userId) %>% 
  summarize(b_u = mean(rating)) %>% 
  filter(n()>=100) %>%
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30 ,color = "green", fill="blue")+
  ggtitle("User effect hist")+
  theme_light()

```

 
 
```{r}
bu <-edx_train %>%
  left_join(bi, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu_hat - b_i))
```
 
 

 
```{r}
predicted_ratings <- edx_test %>% 
  left_join(bi, by = "movieId") %>%
  left_join(bu, by = "userId") %>%
  mutate(pred = mu_hat + b_i + b_u) %>%
  pull(pred)
```
 
 
 
```{r}
user_rmse <- RMSE(predicted_ratings, edx_test$rating)
rmse_results <- bind_rows(rmse_results, 
                tibble(method = "Avg+Movie+user " ,
                                   RMSE = user_rmse))
rmse_results %>% knitr::kable()
```
 
 
 
####  model 4 Regularization :

regularization permits us to penalize large estimates that come from small sample sizes. by add a penalty for large values of b to the sum of squares equations that we minimize.


$$\hat{b}_i(\lambda) = \frac{1}{\lambda
+ n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)$$
 
 
 
 *lambda is a tuning parameter.We can use cross-validation to choose it.*
 
 
```{r}
lambdas <- seq(0, 10, 0.5)
mu <- mean(edx_train$rating)
just_the_sum <- edx_train %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())
rmses <- sapply(lambdas, function(l){
  predicted_ratings <- edx_test %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    .$pred
  return(RMSE(predicted_ratings, edx_test$rating))
})

```



```{r}
qplot(lambdas, rmses,color=I("blue")) 
```


```{r}
lambdas[which.min(rmses)]
```


We can also use regularization to estimate the user effect

```{r}
lambdas <- seq(0, 10, 0.5)
rmses <- sapply(lambdas, function(x){
  mu <- mean(edx_train$rating)
  b_i <- edx_train %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+x))
  b_u <- edx_train %>% 
    left_join(b_i, by= "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+x))
  predicted_ratings <- edx_test %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, edx_test$rating))
})

qplot(lambdas, rmses, color=I("blue"))  

```



```{r}
lambda <- lambdas[which.min(rmses)]
lambda
```


 
```{r}
rmse_results <- bind_rows(rmse_results,
                          tibble(method="Regularized Movie  User ",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```
 
 
 
 
####  model 5 matrix factorization:


```{r}
# Create a model object  by calling Reco()
library(recosystem)
set.seed(111, sample.kind="Rounding")

edx_train_reco <- with(edx_train, data_memory(user_index = 
                         userId, item_index = movieId, rating = rating ,
                         package = "recosystem" ))
edx_test_reco <- with(edx_test, data_memory(user_index = userId,
                   item_index = movieId, rating = rating, 
                   package = "recosystem"))


r <- Reco()



p_reco <- r$tune(edx_train_reco, opts = list(dim = c(10,20, 30),
                                             costp_l2 = c(0.1, 0.5),
                                             costq_l2 = c(0.1, 0.5),
                                             lrate = c(0.1, 0.5),
                                             nthread = 4,
                                             niter = 10))
# Train the model by calling the train()
r$train(edx_train_reco, opts = c(p_reco$min, nthread = 4, niter = 30))


```

 
 
 
```{r}
results_reco <- r$predict(edx_test_reco, out_memory())

# our RMSE for this model 

reco_rmse <- RMSE(results_reco, edx_test$rating)
rmse_results  <- bind_rows(rmse_results , tibble(method = 
                         "matrix factorization ", RMSE = reco_rmse))
 
rmse_results %>% knitr::kable()
```
 
 
 
####  applying model 5 on final_holdout_test set:


```{r}
set.seed(111, sample.kind="Rounding")

edx_reco <- with(edx, data_memory(user_index = 
                 userId, item_index = movieId, rating = rating ,
                 package = "recosystem" ))
 

final_holdout_test_reco <- with( final_holdout_test,
                              data_memory(user_index =userId,
                              item_index = movieId, rating = rating, 
                               package = "recosystem"))



r <- Reco()


p_final_reco <- r$tune(edx_reco, opts = list(dim = c(10,20, 30),
                                             costp_l2 = c(0.1, 0.5),
                                             costq_l2 = c(0.1, 0.5),
                                             lrate = c(0.1, 0.5),
                                             nthread = 4,
                                             niter = 10))
# Train the  model  on edx set by calling the train()
r$train(edx_reco, opts = c(p_final_reco$min, nthread = 4, niter = 30))

```



```{r}
final_results_reco <- r$predict(final_holdout_test_reco, out_memory())
```

 
 
####  our final RMSE for this model on final_hold_out set:



```{r}
final_reco_rmse <- RMSE(final_results_reco, final_holdout_test $rating)
rmse_results  <- bind_rows(rmse_results , tibble(method = 
                            "final matrix factorization ",
                               RMSE = final_reco_rmse))

 
rmse_results %>% knitr::kable()
```
 
 
 
 
 
 
####  Citations:


Irizarry, Rafael A. -Introduction to Data Science-

Yixuan Qiu - recosystem: Recommender System Using Parallel Matrix               Factorization -







 
 
